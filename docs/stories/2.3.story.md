---
epic: 2
story-id: 3
title: "Asynchronous Upload Processing"
status: "Draft"
---

### 1. Story

As a user who has uploaded a file, I want the system to create an import job and start processing it in the background, so that my UI is not blocked and I can do other things.

### 2. Acceptance Criteria

1.  After a file is successfully uploaded to Supabase Storage (from the frontend), the frontend makes a call to a new backend endpoint, notifying it of the file's path.
2.  A new backend endpoint `POST /api/v1/import-jobs` is created to handle this notification.
3.  Upon receiving the notification, the backend creates a new `ImportJob` record in the database with an initial status of 'uploading' or 'parsing'.
4.  The backend then enqueues a background task (using GCP Cloud Tasks) to handle the actual OCR parsing of the file.
5.  The backend immediately returns a `202 Accepted` response to the frontend, including the newly created `ImportJob` details.
6.  The frontend receives the response and can use the job ID to track the status later (e.g., by navigating to the review page).

### 3. Dev Notes

#### Previous Story Insights
*   This story is the logical continuation of the UI created in Story 2.1. It implements the core logic that happens when a file is dropped into the `FileUploadZone`.

#### Data Models
*   **`ImportJob`**: The `POST /import-jobs` endpoint will create a new record in this table.
    *   `[Source: architecture.md#4.2 Model: ImportJob]`

#### API Specifications
*   **`POST /api/v1/import-jobs`**: This is the primary endpoint to implement. It accepts a payload containing the `file_path` of the uploaded file in Supabase Storage.
    *   `[Source: architecture.md#5.1 REST API Specification (OpenAPI)]`
*   **GCP Cloud Tasks**: The backend will need to be configured with the GCP client library for Cloud Tasks. It will push a new task to a pre-configured queue. The task's payload should contain the `import_job_id`.
    *   `[Source: architecture.md#6.1 Component List]`
*   **Asynchronous Workflow**: This story implements the first half of the core async workflow. The actual processing triggered by the task is out of scope for this story and will be handled next.
    *   `[Source: architecture.md#8.1 Workflow: Asynchronous Document Processing]`

#### Component Specifications
*   **FileUploadZone**: The component from Story 2.1 will be modified.
    *   The file upload logic needs to be implemented. It should first upload the file directly to Supabase Storage using the client library.
    *   On successful upload to Storage, it will then make the `POST` call to our backend API with the returned file path.
    *   After receiving the `202 Accepted` response, it should trigger a navigation to the review/mapping page for that new job ID.
    *   `[Source: front-end-spec.md#4.1 Key Screen Layouts]`

#### File Locations
*   **Frontend Component**: `apps/web/src/components/upload/FileUploadZone.tsx` will be modified.
*   **Backend API Endpoint**: The `apps/api/routers/import_jobs.py` file (created in 2.1) will be updated to include the `POST` endpoint.
*   **Backend Task Logic**: A new utility or service file for interacting with GCP Cloud Tasks should be created, e.g., `apps/api/services/task_queue.py`.

#### Testing Requirements
*   **Backend (Pytest)**:
    *   Write unit tests for the `POST /import-jobs` endpoint.
    *   Mock the database insertion and the GCP Cloud Tasks client.
    *   Verify that a new job is created and a task is enqueued with the correct payload.
*   **Frontend (Jest/RTL)**:
    *   Update tests for `FileUploadZone.tsx`.
    *   Mock the Supabase Storage upload and the backend API call.
    *   Verify that a successful upload triggers the API call and a subsequent navigation.

### 4. Tasks / Subtasks

1.  **Frontend: Implement File Upload to Storage (AC: 1)**
    *   [ ] In `FileUploadZone.tsx`, implement the logic to upload the selected file to a user-specific bucket in Supabase Storage using the JS client.
2.  **Backend: Implement `POST /import-jobs` Endpoint (AC: 2, 3)**
    *   [ ] In `apps/api/routers/import_jobs.py`, create the `POST` endpoint.
    *   [ ] The endpoint should accept a `file_path`.
    *   [ ] It should create a new `ImportJob` in the database with the `file_path` and the authenticated `user_id`.
3.  **Backend: Implement Task Enqueueing (AC: 4)**
    *   [ ] Add the GCP Cloud Tasks client library to the backend dependencies.
    *   [ ] Create a service/utility to handle task creation.
    *   [ ] In the `POST /import-jobs` endpoint, after creating the database record, call this service to enqueue a new task. The task's target URL should point to a new, internal-only endpoint for processing, and its payload must include the `import_job_id`.
4.  **Frontend: Connect UI to Backend (AC: 1, 5, 6)**
    *   [ ] After the file upload to Supabase Storage is successful, make a `POST` call from `FileUploadZone.tsx` to the backend endpoint with the file path.
    *   [ ] On receiving the `202` response, use the `jobId` from the response body to navigate the user to the review page (e.g., `/jobs/{jobId}`).
5.  **Write Tests**
    *   [ ] Implement backend unit tests for the endpoint and task enqueueing logic.
    *   [ ] Implement frontend unit tests for the file upload and API call flow.

### 5. QA Results
*   *Pending*
